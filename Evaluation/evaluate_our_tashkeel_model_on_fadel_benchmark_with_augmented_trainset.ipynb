{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1243c826-4d02-4367-8bbf-9cfc1ff66bed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35,
     "referenced_widgets": [
      "f89c4202cc5447d68e6b6f4e5153f07c"
     ]
    },
    "collapsed": true,
    "id": "1243c826-4d02-4367-8bbf-9cfc1ff66bed",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "df914146-458a-4eb7-c484-30ebed8bcabd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abderrahman.skiredj/.conda/envs/skiredj/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 496/496 [00:00<00:00, 5.01MB/s]\n",
      "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/618k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 618k/618k [00:00<00:00, 1.14MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 267.56it/s]\n",
      "Generating train split: 100%|██████████| 2202/2202 [00:00<00:00, 37837.14 examples/s]\n",
      "Downloading readme: 100%|██████████| 494/494 [00:00<00:00, 4.57MB/s]\n",
      "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/384k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 384k/384k [00:00<00:00, 1.03MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 297.15it/s]\n",
      "Generating train split: 100%|██████████| 298/298 [00:00<00:00, 7217.40 examples/s]\n",
      "Running tokenizer on train dataset:   0%|          | 0/298 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (744 > 512). Running this sequence through the model will result in indexing errors\n",
      "Running tokenizer on train dataset: 100%|██████████| 298/298 [00:00<00:00, 458.94 examples/s]\n",
      "Running tokenizer on train dataset: 100%|██████████| 2202/2202 [00:00<00:00, 2206.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, BertForTokenClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "model_name = \"AbderrahmanSkiredj1/Ad-dabit-fadel-augmented\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "#test = load_dataset(\"AbderrahmanSkiredj1/fadel_test_sentences_sizeok_as_token_classif\")\n",
    "test_short = load_dataset('AbderrahmanSkiredj1/fadel_test_short_sentences')\n",
    "test_long = load_dataset('AbderrahmanSkiredj1/fadel_test_long_sentences')\n",
    "#print(test)\n",
    "\n",
    "text_column_name = \"tokens\"\n",
    "label_column_name = \"labels\"\n",
    "b_to_i_label = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "label_to_id = {'X': -100,\n",
    " 'تطويل': 1,\n",
    " 'سكون': 2,\n",
    " 'شدة': 3,\n",
    " 'شدة ضمة': 4,\n",
    " 'شدة ضمتان': 5,\n",
    " 'شدة فتحة': 6,\n",
    " 'شدة فتحتان': 7,\n",
    " 'شدة كسرة': 8,\n",
    " 'شدة كسرتان': 9,\n",
    " 'ضمة': 10,\n",
    " 'ضمتان': 11,\n",
    " 'فتحة': 12,\n",
    " 'فتحتان': 13,\n",
    " 'كسرة': 14,\n",
    " 'كسرتان': 15}\n",
    "label_list = ['X',\n",
    " 'تطويل',\n",
    " 'سكون',\n",
    " 'شدة',\n",
    " 'شدة ضمة',\n",
    " 'شدة ضمتان',\n",
    " 'شدة فتحة',\n",
    " 'شدة فتحتان',\n",
    " 'شدة كسرة',\n",
    " 'شدة كسرتان',\n",
    " 'ضمة',\n",
    " 'ضمتان',\n",
    " 'فتحة',\n",
    " 'فتحتان',\n",
    " 'كسرة',\n",
    " 'كسرتان']\n",
    "num_labels = len(label_list)\n",
    "padding = \"max_length\"\n",
    "padding = \"longest\"\n",
    "padding=False\n",
    "max_seq_length=512\n",
    "label_all_tokens=False\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=padding,\n",
    "        #truncation=True,\n",
    "        #max_length=max_seq_length,\n",
    "        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[label_column_name]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_to_id[label[word_idx]])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                if label_all_tokens:\n",
    "                    label_ids.append(b_to_i_label[label_to_id[label[word_idx]]])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "test_long = test_long.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    #load_from_cache_file=True,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    ")\n",
    "test_short = test_short.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    #load_from_cache_file=True,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18451e09-6711-4498-93d2-ffb701fe0668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 2202\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdaeba88-5f52-497a-8044-277472166a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 298\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "R3FNlAQ7lCzC",
   "metadata": {
    "id": "R3FNlAQ7lCzC"
   },
   "outputs": [],
   "source": [
    "test_long = test_long['train']\n",
    "test_short = test_short['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "S3ZbPW56gKEx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3ZbPW56gKEx",
    "outputId": "5bf7cf0c-2752-4942-c6df-3181c7912378"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "539"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_long[200]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "K5HiXB8h9kUO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5HiXB8h9kUO",
    "outputId": "c02398a8-d504-4001-cb0d-79a1a65e41bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_short[200]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ca2e8b4-dea1-4d93-a963-f071c8678b6a",
   "metadata": {
    "id": "3ca2e8b4-dea1-4d93-a963-f071c8678b6a"
   },
   "outputs": [],
   "source": [
    "def compute_der_wer_numbers(zipped_list):\n",
    "    # Reinitialize the list and variables with correct logic for handling consecutive -100 and accurate DER and WER calculation\n",
    "    total_characters = 0\n",
    "    incorrect_characters = 0\n",
    "    total_words = 0\n",
    "    incorrect_words = 0\n",
    "    current_word_correct = True\n",
    "    # This time, ensuring we only count a new word when we've actually seen diacritics following -100\n",
    "    just_seen_100 = False  # Indicates we've just seen a -100, helps in handling consecutive -100s\n",
    "\n",
    "    for gt, pred in zipped_list:\n",
    "        if gt == -100:\n",
    "            if not just_seen_100:  # Only consider starting a new word if we haven't just seen a -100\n",
    "                just_seen_100 = True\n",
    "                if not current_word_correct:  # If the last word (if any) was incorrect\n",
    "                    incorrect_words += 1\n",
    "                current_word_correct = True  # Assume the new word is correct until proven otherwise\n",
    "        else:  # We are in a word with diacritics\n",
    "            if just_seen_100:  # Transitioning from -100 to diacritics, mark a new word\n",
    "                total_words += 1\n",
    "                just_seen_100 = False  # Reset since we've now started a new word\n",
    "            total_characters += 1\n",
    "            if gt != pred:\n",
    "                incorrect_characters += 1\n",
    "                current_word_correct = False\n",
    "\n",
    "    # After loop, if the last word was incorrect and it was a real word (not just a -100), account for it\n",
    "    if not just_seen_100 and not current_word_correct:  # Check for the last word if it was incorrect\n",
    "        incorrect_words += 1\n",
    "\n",
    "    # Recalculate DER and WER with corrected logic\n",
    "    #der = incorrect_characters / total_characters if total_characters > 0 else 0\n",
    "    #wer = incorrect_words / total_words if total_words > 0 else 0\n",
    "    return incorrect_characters, total_characters, incorrect_words, total_words\n",
    "\n",
    "def compute_error_numbers_for_sample_original(input_ids, labels, model):\n",
    "    inp = torch.tensor(input_ids)\n",
    "    inp = inp.unsqueeze(0)\n",
    "    inp = inp.to(device)\n",
    "    o3 = model(inp)\n",
    "    lg3 = o3.logits\n",
    "    predicted_labels = torch.argmax(lg3, dim=-1)\n",
    "    zi = list(zip(labels, predicted_labels.tolist()[0]))\n",
    "    return compute_der_wer_numbers(zi)\n",
    "\n",
    "def compute_error_numbers_for_sample(input_ids, labels, model, step=20, no_slide = False):\n",
    "    if len(input_ids) < 512 or no_slide:\n",
    "        inp = torch.tensor(input_ids)\n",
    "        inp = inp.unsqueeze(0)\n",
    "        inp = inp.to(device)\n",
    "        o3 = model(inp)\n",
    "        lg3 = o3.logits\n",
    "        predicted_labels = torch.argmax(lg3, dim=-1)\n",
    "        zi = list(zip(labels, predicted_labels.tolist()[0]))\n",
    "        return compute_der_wer_numbers(zi)\n",
    "    else:\n",
    "        return compute_error_numbers_for_sample_with_slides(input_ids, labels, model, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b61414ac-fb5a-488b-90e5-4663c96d9f78",
   "metadata": {
    "id": "b61414ac-fb5a-488b-90e5-4663c96d9f78"
   },
   "outputs": [],
   "source": [
    "def compute_error_numbers_for_sample_with_slides(input_ids, labels, model, step):\n",
    "    sublists, input_id_mapping = get_windows_of_512_from_long_inputids(input_ids, labels, step)\n",
    "    predictions_by_sublists = dict()\n",
    "    for i, sub in enumerate(sublists):\n",
    "        subinput_ids = sub['input_ids']+[3]\n",
    "        sublabels = sub['labels']+[-100]\n",
    "        inp = torch.tensor(subinput_ids)\n",
    "        inp = inp.unsqueeze(0)\n",
    "        inp = inp.to(device)\n",
    "        o3 = model(inp)\n",
    "        lg3 = o3.logits\n",
    "        predicted_labels = torch.argmax(lg3, dim=-1)\n",
    "        #subzi = list(zip(sublabels, predicted_labels.tolist()[0]))\n",
    "        #all_zipped.append(subzi)\n",
    "        predictions_by_sublists[i] = predicted_labels.tolist()[0]\n",
    "    final_predictions = []\n",
    "    for k in input_id_mapping:\n",
    "        predicted_labels_for_k = []\n",
    "        for sub in input_id_mapping[k]:\n",
    "            indices_of_k_in_sub = input_id_mapping[k][sub]\n",
    "            predicted_labels_for_k.append(predictions_by_sublists[sub][indices_of_k_in_sub])\n",
    "        final_predictions.append(max(predicted_labels_for_k, key = predicted_labels_for_k.count))\n",
    "    zi = list(zip(labels, final_predictions))\n",
    "    return compute_der_wer_numbers(zi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "y1SIK7rhG2kg",
   "metadata": {
    "id": "y1SIK7rhG2kg"
   },
   "outputs": [],
   "source": [
    "def get_windows_of_512_from_long_inputids(input_ids, labels, step):\n",
    "    my_list = input_ids\n",
    "    sublists = []\n",
    "    input_id_mapping = {}\n",
    "    start_index = 0\n",
    "    sublist_index = 0  # Initialize a counter for the sublist index\n",
    "    have_we_reached_end = False\n",
    "    while start_index < len(my_list):\n",
    "        temp_sum = 0\n",
    "        temp_list = []\n",
    "        i = start_index\n",
    "        while i < len(my_list):\n",
    "            is_next_one_a_four = False\n",
    "            if temp_sum + 1 > 500:\n",
    "                break\n",
    "            j = i+1\n",
    "            if j<len(my_list):\n",
    "                if my_list[j]==4:\n",
    "                    is_next_one_a_four = True\n",
    "                    while j+1 < len(my_list):\n",
    "                        if my_list[j+1]==4:\n",
    "                            j += 1\n",
    "                        else:\n",
    "                            break\n",
    "            #(i + j-i ==> +1 + j-i)\n",
    "            if not is_next_one_a_four:\n",
    "                temp_sum += 1\n",
    "                temp_list.append(my_list[i])\n",
    "                if i not in input_id_mapping:\n",
    "                    #input_id_mapping[i] = [sublist_index]\n",
    "                    input_id_mapping[i] = {sublist_index: len(temp_list) - 1}\n",
    "                else:\n",
    "                    #input_id_mapping[i].append(sublist_index)\n",
    "                    input_id_mapping[i][sublist_index] = len(temp_list) - 1\n",
    "                i += 1\n",
    "            else:\n",
    "                temp_sum += 1+j-i\n",
    "                temp_list.extend(my_list[i:j+1])\n",
    "                for k in range(i, j + 1):\n",
    "                    if k not in input_id_mapping:\n",
    "                        input_id_mapping[k] = {sublist_index: len(temp_list) - (j + 1 - k)}\n",
    "                        #input_id_mapping[k] = [sublist_index]\n",
    "                    else:\n",
    "                        input_id_mapping[k][sublist_index] = len(temp_list) - (j + 1 - k)\n",
    "                        #input_id_mapping[k].append(sublist_index)\n",
    "                i = j+1\n",
    "            if i>len(my_list)-1:\n",
    "                have_we_reached_end = True\n",
    "\n",
    "        sublabels = labels[start_index:start_index+len(temp_list)]\n",
    "        sublists.append({\"input_ids\": temp_list, \"labels\": sublabels})\n",
    "        sublist_index += 1\n",
    "\n",
    "        start_index += step  # Slide by the step size\n",
    "\n",
    "        # Ensure start_index does not point to a \"4\", and skip any \"4\"s if necessary\n",
    "        while start_index < len(my_list) and my_list[start_index] == 4:\n",
    "            start_index += 1  # Skip \"4\"\n",
    "\n",
    "\n",
    "        # Make sure start_index does not exceed the list length after adjustment\n",
    "        if start_index >= len(my_list):\n",
    "            break  # Exit the loop if we have reached or exceeded the list length\n",
    "\n",
    "        if have_we_reached_end:\n",
    "            break\n",
    "    return sublists, input_id_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5d4cb03-0a8a-45d8-96ac-db94d0786c74",
   "metadata": {
    "id": "c5d4cb03-0a8a-45d8-96ac-db94d0786c74",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(64000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = BertForTokenClassification.from_pretrained(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dc94196-a297-47da-9ffd-e56e0717dd56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dc94196-a297-47da-9ffd-e56e0717dd56",
    "outputId": "8a4bc58b-5369-44a9-a43d-e2d9e7c27e20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2202/2202 [00:20<00:00, 106.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "total_nb_words= 0\n",
    "total_nb_characters = 0\n",
    "incorrect_words=0\n",
    "incorrect_characters=0\n",
    "for sample in tqdm(test_short):\n",
    "    delta_incorrect_characters, delta_total_characters, delta_incorrect_words, delta_total_words = compute_error_numbers_for_sample(sample['input_ids'], sample['labels'], model, step=10, no_slide=True)\n",
    "    total_nb_words += delta_total_words\n",
    "    total_nb_characters += delta_total_characters\n",
    "    incorrect_words += delta_incorrect_words\n",
    "    incorrect_characters += delta_incorrect_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "Pv9i_i6R_qx2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pv9i_i6R_qx2",
    "outputId": "dc176a54-0105-41b7-9624-885440c65355"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8141188501669783, 2.523764258555133)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*incorrect_characters/total_nb_characters, 100*incorrect_words/total_nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "rNJuWHUU-LGY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rNJuWHUU-LGY",
    "outputId": "cd62ed1a-2628-4139-e9c6-26563c9aa6f6"
   },
   "outputs": [],
   "source": [
    "initial_total_nb_words, initial_total_nb_characters, initial_incorrect_words, initial_incorrect_characters = total_nb_words, total_nb_characters, incorrect_words, incorrect_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4-MlJQ-j-LIy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-MlJQ-j-LIy",
    "outputId": "67c42f7d-e264-42f7-aad7-7c3f8596a402"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [01:17<00:00,  3.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "total_nb_words= initial_total_nb_words\n",
    "total_nb_characters = initial_total_nb_characters\n",
    "incorrect_words=initial_incorrect_words\n",
    "incorrect_characters=initial_incorrect_characters\n",
    "for sample in tqdm(test_long):\n",
    "    delta_incorrect_characters, delta_total_characters, delta_incorrect_words, delta_total_words = compute_error_numbers_for_sample(sample['input_ids'], sample['labels'], model, step=10)\n",
    "    total_nb_words += delta_total_words\n",
    "    total_nb_characters += delta_total_characters\n",
    "    incorrect_words += delta_incorrect_words\n",
    "    incorrect_characters += delta_incorrect_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "547g7DPq-LK9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "547g7DPq-LK9",
    "outputId": "71193e03-8392-4111-db19-5497425b60e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7992114007305863, 2.4867769604389203)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*incorrect_characters/total_nb_characters, 100*incorrect_words/total_nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "UMebCk3k-LNh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMebCk3k-LNh",
    "outputId": "e2d1b045-da8b-4ee8-dd9b-afb417615a99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:13<00:00, 21.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8046353789592093, 2.505566359459993)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "total_nb_words= initial_total_nb_words\n",
    "total_nb_characters = initial_total_nb_characters\n",
    "incorrect_words=initial_incorrect_words\n",
    "incorrect_characters=initial_incorrect_characters\n",
    "for sample in tqdm(test_long):\n",
    "    delta_incorrect_characters, delta_total_characters, delta_incorrect_words, delta_total_words = compute_error_numbers_for_sample(sample['input_ids'], sample['labels'], model, step=100)\n",
    "    total_nb_words += delta_total_words\n",
    "    total_nb_characters += delta_total_characters\n",
    "    incorrect_words += delta_incorrect_words\n",
    "    incorrect_characters += delta_incorrect_characters\n",
    "100*incorrect_characters/total_nb_characters, 100*incorrect_words/total_nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "-L61Cuwc-LQZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-L61Cuwc-LQZ",
    "outputId": "fd0a592a-98fa-4537-b7f8-68d578046194"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:25<00:00, 11.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.805342854380334, 2.504626889508939)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "total_nb_words= initial_total_nb_words\n",
    "total_nb_characters = initial_total_nb_characters\n",
    "incorrect_words=initial_incorrect_words\n",
    "incorrect_characters=initial_incorrect_characters\n",
    "for sample in tqdm(test_long):\n",
    "    delta_incorrect_characters, delta_total_characters, delta_incorrect_words, delta_total_words = compute_error_numbers_for_sample(sample['input_ids'], sample['labels'], model, step=40)\n",
    "    total_nb_words += delta_total_words\n",
    "    total_nb_characters += delta_total_characters\n",
    "    incorrect_words += delta_incorrect_words\n",
    "    incorrect_characters += delta_incorrect_characters\n",
    "100*incorrect_characters/total_nb_characters, 100*incorrect_words/total_nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fnAdipJpGKAX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fnAdipJpGKAX",
    "outputId": "8d9cab43-8108-45dd-87ff-210273a87131"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [02:08<00:00,  2.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8041637286784595, 2.504626889508939)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "total_nb_words= initial_total_nb_words\n",
    "total_nb_characters = initial_total_nb_characters\n",
    "incorrect_words=initial_incorrect_words\n",
    "incorrect_characters=initial_incorrect_characters\n",
    "for sample in tqdm(test_long):\n",
    "    delta_incorrect_characters, delta_total_characters, delta_incorrect_words, delta_total_words = compute_error_numbers_for_sample(sample['input_ids'], sample['labels'], model, step=5)\n",
    "    total_nb_words += delta_total_words\n",
    "    total_nb_characters += delta_total_characters\n",
    "    incorrect_words += delta_incorrect_words\n",
    "    incorrect_characters += delta_incorrect_characters\n",
    "100*incorrect_characters/total_nb_characters, 100*incorrect_words/total_nb_words"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
